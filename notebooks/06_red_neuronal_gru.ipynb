{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93e4bed2",
   "metadata": {},
   "source": [
    "# Red Neuronal con GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a4927e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_length</th>\n",
       "      <th>num_hashtags</th>\n",
       "      <th>num_mentions</th>\n",
       "      <th>num_uppercase_per_word</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>has_url</th>\n",
       "      <th>country</th>\n",
       "      <th>keyword</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2721</th>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.18755</td>\n",
       "      <td>1</td>\n",
       "      <td>unknown</td>\n",
       "      <td>devastated</td>\n",
       "      <td>Obama declares disaster for typhoon-devastated...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2259</th>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.55135</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>deluged</td>\n",
       "      <td>Businesses are deluged with invzices. Make you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1815</th>\n",
       "      <td>136</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>0.27205</td>\n",
       "      <td>1</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>crashed</td>\n",
       "      <td>Neil_Eastwood77: I AM A KNOBHEAD!! Bin Laden f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>139</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>1</td>\n",
       "      <td>unknown</td>\n",
       "      <td>blazing</td>\n",
       "      <td>Morgan Silver Dollar 1880 S Gem BU DMPL Cameo ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7216</th>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.78595</td>\n",
       "      <td>0</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>weapons</td>\n",
       "      <td>@danagould @WaynesterAtl I agree with backgrou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      text_length  num_hashtags  num_mentions  num_uppercase_per_word  \\\n",
       "2721           87             1             0                1.000000   \n",
       "2259          132             0             0                0.083333   \n",
       "1815          136             0             0                1.333333   \n",
       "682           139             0             0                1.666667   \n",
       "7216          121             0             2                0.210526   \n",
       "\n",
       "      sentiment_score  has_url                   country     keyword  \\\n",
       "2721          0.18755        1                   unknown  devastated   \n",
       "2259          0.55135        0                   unknown     deluged   \n",
       "1815          0.27205        1            United Kingdom     crashed   \n",
       "682           0.50000        1                   unknown     blazing   \n",
       "7216          0.78595        0  United States of America     weapons   \n",
       "\n",
       "                                                   text  \n",
       "2721  Obama declares disaster for typhoon-devastated...  \n",
       "2259  Businesses are deluged with invzices. Make you...  \n",
       "1815  Neil_Eastwood77: I AM A KNOBHEAD!! Bin Laden f...  \n",
       "682   Morgan Silver Dollar 1880 S Gem BU DMPL Cameo ...  \n",
       "7216  @danagould @WaynesterAtl I agree with backgrou...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "import json\n",
    "\n",
    "DATA_DIR = Path('../data/nlp-getting-started')\n",
    "TRAIN_PATH = DATA_DIR / 'train.csv'\n",
    "TEST_PATH = DATA_DIR / 'test.csv'\n",
    "LOCATION_TO_COUNTRY_PATH = Path('../data/location_to_country.json')\n",
    "RANDOM_SEED = 27\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "test_df = pd.read_csv(TEST_PATH)\n",
    "\n",
    "with open(LOCATION_TO_COUNTRY_PATH, 'r', encoding='utf-8') as f:\n",
    "    location_to_country = json.load(f)\n",
    "\n",
    "# Categóricas: 'country',  'keyword'. Después las voy a mean encodear.\n",
    "train_df['country'] = train_df['location'].map(location_to_country).fillna('unknown')\n",
    "test_df['country'] = test_df['location'].map(location_to_country).fillna('unknown')\n",
    "\n",
    "train_df['keyword'] = train_df['keyword'].fillna('missing')\n",
    "test_df['keyword'] = test_df['keyword'].fillna('missing')\n",
    "\n",
    "\n",
    "categorical_features = ['country', 'keyword']\n",
    "\n",
    "# Numéricas: 'text_length', 'num_hashtags', 'num_mentions', 'num_uppercase_per_word', 'sentiment_score', 'has_url'\n",
    "\n",
    "# one hot encoding de 'has_url' a mano\n",
    "train_df['has_url'] = train_df['text'].fillna('').str.contains(r'http[s]?://', regex=True).astype(int)\n",
    "test_df['has_url'] = test_df['text'].fillna('').str.contains(r'http[s]?://', regex=True).astype(int)\n",
    "\n",
    "train_df['text_length'] = train_df['text'].fillna('').str.len()\n",
    "test_df['text_length'] = test_df['text'].fillna('').str.len()\n",
    "\n",
    "train_df['num_hashtags'] = train_df['text'].str.count('#')\n",
    "train_df['num_mentions'] = train_df['text'].str.count('@')\n",
    "\n",
    "test_df['num_hashtags'] = test_df['text'].str.count('#')\n",
    "test_df['num_mentions'] = test_df['text'].str.count('@')\n",
    "\n",
    "def uppercase_per_word(text):\n",
    "    text = str(text)\n",
    "\n",
    "    # Palabras que tengan al menos una letra alfabética\n",
    "    words = [w for w in text.split() if any(ch.isalpha() for ch in w)]\n",
    "    if not words:\n",
    "        return 0.0\n",
    "\n",
    "    # Solo letras alfabéticas, para evitar que cuenten símbolos raros\n",
    "    uppercase_letters = sum(ch.isupper() for ch in text if ch.isalpha())\n",
    "    return uppercase_letters / len(words)\n",
    "\n",
    "\n",
    "train_df['num_uppercase_per_word'] = train_df['text'].apply(uppercase_per_word)\n",
    "test_df['num_uppercase_per_word']  = test_df['text'].apply(uppercase_per_word)\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_sentiment(text):\n",
    "    if pd.isna(text) or text.strip() == '':\n",
    "        return 0.5\n",
    "    compound = analyzer.polarity_scores(text)['compound']\n",
    "    return (compound + 1) / 2\n",
    "\n",
    "train_df['sentiment_score'] = train_df['text'].apply(get_sentiment)\n",
    "test_df['sentiment_score'] = test_df['text'].apply(get_sentiment)\n",
    "\n",
    "numeric_features = ['text_length', 'num_hashtags', 'num_mentions', 'num_uppercase_per_word', 'sentiment_score', 'has_url']\n",
    "\n",
    "embedding_feature = 'text'\n",
    "\n",
    "# 1. Separar features y target\n",
    "X = train_df[numeric_features + categorical_features + [embedding_feature]].copy()\n",
    "y = train_df['target'].copy()\n",
    "\n",
    "# 2. Split estratificado train/validation (80/20)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_SEED, stratify=y\n",
    ")\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a19a6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def kfold_target_encoding(train_series, target_series, n_splits=5, random_state=RANDOM_SEED):\n",
    "    encoded = pd.Series(np.nan, index=train_series.index, dtype=float)\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    global_mean = target_series.mean()\n",
    "\n",
    "    for tr_idx, val_idx in kf.split(train_series):\n",
    "        fold_df = pd.DataFrame({\n",
    "            'feature': train_series.iloc[tr_idx],\n",
    "            'target': target_series.iloc[tr_idx]\n",
    "        })\n",
    "        means = fold_df.groupby('feature')['target'].mean()\n",
    "        encoded.iloc[val_idx] = train_series.iloc[val_idx].map(means)\n",
    "\n",
    "    encoded.fillna(global_mean, inplace=True)\n",
    "\n",
    "    full_df = pd.DataFrame({'feature': train_series, 'target': target_series})\n",
    "    mapping = full_df.groupby('feature')['target'].mean()\n",
    "\n",
    "    return encoded, mapping, global_mean\n",
    "\n",
    "mean_encoded_features = []\n",
    "\n",
    "for col in ['country', 'keyword']:\n",
    "    train_encoded, mapping, global_mean = kfold_target_encoding(\n",
    "        X_train[col], y_train\n",
    "    )\n",
    "    new_col = f'{col}_target_mean'\n",
    "    X_train[new_col] = train_encoded\n",
    "    X_val[new_col]   = X_val[col].map(mapping).fillna(global_mean)\n",
    "    test_df[new_col] = test_df[col].map(mapping).fillna(global_mean)\n",
    "    mean_encoded_features.append(new_col)\n",
    "\n",
    "# Actualizamos lista de numéricas\n",
    "numeric_features = numeric_features + mean_encoded_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a01a38",
   "metadata": {},
   "source": [
    "Normalizamos todas las features para darle a la Red neuronal un mejor input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53e1ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_num = scaler.fit_transform(X_train[numeric_features])\n",
    "X_val_num   = scaler.transform(X_val[numeric_features])\n",
    "X_test_num  = scaler.transform(test_df[numeric_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abbd738",
   "metadata": {},
   "source": [
    "Preparamos el texto para la GRU (tokenizer + padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773c88d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "text_col = embedding_feature  # 'text'\n",
    "MAX_WORDS = 10000\n",
    "MAX_LEN   = 40  # tweets, podés cambiar tras ver histograma de longitudes\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train[text_col].fillna(''))\n",
    "\n",
    "X_train_seq = pad_sequences(\n",
    "    tokenizer.texts_to_sequences(X_train[text_col].fillna('')),\n",
    "    maxlen=MAX_LEN\n",
    ")\n",
    "X_val_seq = pad_sequences(\n",
    "    tokenizer.texts_to_sequences(X_val[text_col].fillna('')),\n",
    "    maxlen=MAX_LEN\n",
    ")\n",
    "X_test_seq = pad_sequences(\n",
    "    tokenizer.texts_to_sequences(test_df[text_col].fillna('')),\n",
    "    maxlen=MAX_LEN\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
