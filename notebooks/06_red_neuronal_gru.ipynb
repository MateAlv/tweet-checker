{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93e4bed2",
   "metadata": {},
   "source": [
    "# Red Neuronal con GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a4927e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_length</th>\n",
       "      <th>num_hashtags</th>\n",
       "      <th>num_mentions</th>\n",
       "      <th>num_uppercase_per_word</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>has_url</th>\n",
       "      <th>country</th>\n",
       "      <th>keyword</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2721</th>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.18755</td>\n",
       "      <td>1</td>\n",
       "      <td>unknown</td>\n",
       "      <td>devastated</td>\n",
       "      <td>Obama declares disaster for typhoon-devastated...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2259</th>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.55135</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>deluged</td>\n",
       "      <td>Businesses are deluged with invzices. Make you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1815</th>\n",
       "      <td>136</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>0.27205</td>\n",
       "      <td>1</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>crashed</td>\n",
       "      <td>Neil_Eastwood77: I AM A KNOBHEAD!! Bin Laden f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>139</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>1</td>\n",
       "      <td>unknown</td>\n",
       "      <td>blazing</td>\n",
       "      <td>Morgan Silver Dollar 1880 S Gem BU DMPL Cameo ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7216</th>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.78595</td>\n",
       "      <td>0</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>weapons</td>\n",
       "      <td>@danagould @WaynesterAtl I agree with backgrou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      text_length  num_hashtags  num_mentions  num_uppercase_per_word  \\\n",
       "2721           87             1             0                1.000000   \n",
       "2259          132             0             0                0.083333   \n",
       "1815          136             0             0                1.333333   \n",
       "682           139             0             0                1.666667   \n",
       "7216          121             0             2                0.210526   \n",
       "\n",
       "      sentiment_score  has_url                   country     keyword  \\\n",
       "2721          0.18755        1                   unknown  devastated   \n",
       "2259          0.55135        0                   unknown     deluged   \n",
       "1815          0.27205        1            United Kingdom     crashed   \n",
       "682           0.50000        1                   unknown     blazing   \n",
       "7216          0.78595        0  United States of America     weapons   \n",
       "\n",
       "                                                   text  \n",
       "2721  Obama declares disaster for typhoon-devastated...  \n",
       "2259  Businesses are deluged with invzices. Make you...  \n",
       "1815  Neil_Eastwood77: I AM A KNOBHEAD!! Bin Laden f...  \n",
       "682   Morgan Silver Dollar 1880 S Gem BU DMPL Cameo ...  \n",
       "7216  @danagould @WaynesterAtl I agree with backgrou...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "import json\n",
    "\n",
    "DATA_DIR = Path('../data/nlp-getting-started')\n",
    "TRAIN_PATH = DATA_DIR / 'train.csv'\n",
    "TEST_PATH = DATA_DIR / 'test.csv'\n",
    "LOCATION_TO_COUNTRY_PATH = Path('../data/location_to_country.json')\n",
    "RANDOM_SEED = 27\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "test_df = pd.read_csv(TEST_PATH)\n",
    "\n",
    "with open(LOCATION_TO_COUNTRY_PATH, 'r', encoding='utf-8') as f:\n",
    "    location_to_country = json.load(f)\n",
    "\n",
    "# Categóricas: 'country',  'keyword'. Después las voy a mean encodear.\n",
    "train_df['country'] = train_df['location'].map(location_to_country).fillna('unknown')\n",
    "test_df['country'] = test_df['location'].map(location_to_country).fillna('unknown')\n",
    "\n",
    "train_df['keyword'] = train_df['keyword'].fillna('missing')\n",
    "test_df['keyword'] = test_df['keyword'].fillna('missing')\n",
    "\n",
    "\n",
    "categorical_features = ['country', 'keyword']\n",
    "\n",
    "# Numéricas: 'text_length', 'num_hashtags', 'num_mentions', 'num_uppercase_per_word', 'sentiment_score', 'has_url'\n",
    "\n",
    "# one hot encoding de 'has_url' a mano\n",
    "train_df['has_url'] = train_df['text'].fillna('').str.contains(r'http[s]?://', regex=True).astype(int)\n",
    "test_df['has_url'] = test_df['text'].fillna('').str.contains(r'http[s]?://', regex=True).astype(int)\n",
    "\n",
    "train_df['text_length'] = train_df['text'].fillna('').str.len()\n",
    "test_df['text_length'] = test_df['text'].fillna('').str.len()\n",
    "\n",
    "train_df['num_hashtags'] = train_df['text'].str.count('#')\n",
    "train_df['num_mentions'] = train_df['text'].str.count('@')\n",
    "\n",
    "test_df['num_hashtags'] = test_df['text'].str.count('#')\n",
    "test_df['num_mentions'] = test_df['text'].str.count('@')\n",
    "\n",
    "def uppercase_per_word(text):\n",
    "    text = str(text)\n",
    "\n",
    "    # Palabras que tengan al menos una letra alfabética\n",
    "    words = [w for w in text.split() if any(ch.isalpha() for ch in w)]\n",
    "    if not words:\n",
    "        return 0.0\n",
    "\n",
    "    # Solo letras alfabéticas, para evitar que cuenten símbolos raros\n",
    "    uppercase_letters = sum(ch.isupper() for ch in text if ch.isalpha())\n",
    "    return uppercase_letters / len(words)\n",
    "\n",
    "\n",
    "train_df['num_uppercase_per_word'] = train_df['text'].apply(uppercase_per_word)\n",
    "test_df['num_uppercase_per_word']  = test_df['text'].apply(uppercase_per_word)\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_sentiment(text):\n",
    "    if pd.isna(text) or text.strip() == '':\n",
    "        return 0.5\n",
    "    compound = analyzer.polarity_scores(text)['compound']\n",
    "    return (compound + 1) / 2\n",
    "\n",
    "train_df['sentiment_score'] = train_df['text'].apply(get_sentiment)\n",
    "test_df['sentiment_score'] = test_df['text'].apply(get_sentiment)\n",
    "\n",
    "numeric_features = ['text_length', 'num_hashtags', 'num_mentions', 'num_uppercase_per_word', 'sentiment_score', 'has_url']\n",
    "\n",
    "embedding_feature = 'text'\n",
    "\n",
    "# 1. Separar features y target\n",
    "X = train_df[numeric_features + categorical_features + [embedding_feature]].copy()\n",
    "y = train_df['target'].copy()\n",
    "\n",
    "# 2. Split estratificado train/validation (80/20)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_SEED, stratify=y\n",
    ")\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a19a6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def kfold_target_encoding(train_series, target_series, n_splits=5, random_state=RANDOM_SEED):\n",
    "    encoded = pd.Series(np.nan, index=train_series.index, dtype=float)\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    global_mean = target_series.mean()\n",
    "\n",
    "    for tr_idx, val_idx in kf.split(train_series):\n",
    "        fold_df = pd.DataFrame({\n",
    "            'feature': train_series.iloc[tr_idx],\n",
    "            'target': target_series.iloc[tr_idx]\n",
    "        })\n",
    "        means = fold_df.groupby('feature')['target'].mean()\n",
    "        encoded.iloc[val_idx] = train_series.iloc[val_idx].map(means)\n",
    "\n",
    "    encoded.fillna(global_mean, inplace=True)\n",
    "\n",
    "    full_df = pd.DataFrame({'feature': train_series, 'target': target_series})\n",
    "    mapping = full_df.groupby('feature')['target'].mean()\n",
    "\n",
    "    return encoded, mapping, global_mean\n",
    "\n",
    "mean_encoded_features = []\n",
    "\n",
    "for col in ['country', 'keyword']:\n",
    "    train_encoded, mapping, global_mean = kfold_target_encoding(\n",
    "        X_train[col], y_train\n",
    "    )\n",
    "    new_col = f'{col}_target_mean'\n",
    "    X_train[new_col] = train_encoded\n",
    "    X_val[new_col]   = X_val[col].map(mapping).fillna(global_mean)\n",
    "    test_df[new_col] = test_df[col].map(mapping).fillna(global_mean)\n",
    "    mean_encoded_features.append(new_col)\n",
    "\n",
    "# Actualizamos lista de numéricas\n",
    "numeric_features = numeric_features + mean_encoded_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a01a38",
   "metadata": {},
   "source": [
    "Normalizamos todas las features para darle a la Red neuronal un mejor input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b53e1ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_num = scaler.fit_transform(X_train[numeric_features])\n",
    "X_val_num   = scaler.transform(X_val[numeric_features])\n",
    "X_test_num  = scaler.transform(test_df[numeric_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9b4d06f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    6090.000000\n",
      "mean       14.863383\n",
      "std         5.747268\n",
      "min         1.000000\n",
      "25%        11.000000\n",
      "50%        15.000000\n",
      "75%        19.000000\n",
      "max        31.000000\n",
      "Name: text, dtype: float64\n",
      "Percentiles:\n",
      "0.75    19.0\n",
      "0.90    22.0\n",
      "0.95    24.0\n",
      "0.99    27.0\n",
      "Name: text, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "tweets_len = X_train[text_col].fillna('').str.split().apply(len)\n",
    "print(tweets_len.describe())\n",
    "print(\"Percentiles:\")\n",
    "print(tweets_len.quantile([0.75, 0.90, 0.95, 0.99]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abbd738",
   "metadata": {},
   "source": [
    "Preparamos el texto para la GRU (tokenizer + padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "773c88d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "text_col = embedding_feature  \n",
    "MAX_WORDS = 10000\n",
    "MAX_LEN   = 30\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train[text_col].fillna(''))\n",
    "\n",
    "X_train_seq = pad_sequences(\n",
    "    tokenizer.texts_to_sequences(X_train[text_col].fillna('')),\n",
    "    maxlen=MAX_LEN\n",
    ")\n",
    "X_val_seq = pad_sequences(\n",
    "    tokenizer.texts_to_sequences(X_val[text_col].fillna('')),\n",
    "    maxlen=MAX_LEN\n",
    ")\n",
    "X_test_seq = pad_sequences(\n",
    "    tokenizer.texts_to_sequences(test_df[text_col].fillna('')),\n",
    "    maxlen=MAX_LEN\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7461761f",
   "metadata": {},
   "source": [
    "Preparado de Tensorflow. Recomendaciones e instalación de internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "52d82b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version: 2.20.0\n",
      "Physical GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Logical GPUs: [LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import mixed_precision\n",
    "from tensorflow.keras.layers import Input, Embedding, GRU, Bidirectional, Dense, Dropout, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"\n",
    "mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "print(\"TF version:\", tf.__version__)\n",
    "print(\"Physical GPUs:\", tf.config.list_physical_devices('GPU'))\n",
    "print(\"Logical GPUs:\", tf.config.experimental.list_logical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626237d5",
   "metadata": {},
   "source": [
    "Me detecta mi GPU 4070ti, genial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a40af63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import GlobalMaxPooling1D\n",
    "\n",
    "# Rama texto\n",
    "text_input = Input(shape=(MAX_LEN,), name=\"text_input\")\n",
    "x = Embedding(MAX_WORDS, 128, name=\"embedding\")(text_input)\n",
    "x = Bidirectional(GRU(96, return_sequences=True, dropout=0.3, recurrent_dropout=0.2))(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dropout(0.35)(x)\n",
    "\n",
    "# Rama numérica\n",
    "num_input = Input(shape=(n_num_features,), name=\"num_input\")\n",
    "n = Dense(64, activation='relu', kernel_regularizer=l2(1e-4))(num_input)\n",
    "n = Dropout(0.25)(n)\n",
    "\n",
    "combined = Concatenate()([x, n])\n",
    "h = Dense(96, activation='relu', kernel_regularizer=l2(1e-4))(combined)\n",
    "h = Dropout(0.3)(h)\n",
    "\n",
    "output = Dense(1, activation='sigmoid', dtype='float32')(h)\n",
    "\n",
    "model = Model(inputs=[text_input, num_input], outputs=output)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-4)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad6601f",
   "metadata": {},
   "source": [
    "Entrenamiento de la red neuronal con GRU y features numéricas adicionales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "059c106e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "191/191 - 7s - 38ms/step - accuracy: 0.6782 - loss: 0.6270 - val_accuracy: 0.7715 - val_loss: 0.5328\n",
      "Epoch 2/40\n",
      "191/191 - 7s - 36ms/step - accuracy: 0.7378 - loss: 0.5532 - val_accuracy: 0.7827 - val_loss: 0.4886\n",
      "Epoch 3/40\n",
      "191/191 - 7s - 36ms/step - accuracy: 0.7798 - loss: 0.4962 - val_accuracy: 0.8168 - val_loss: 0.4319\n",
      "Epoch 4/40\n",
      "191/191 - 7s - 36ms/step - accuracy: 0.8415 - loss: 0.3903 - val_accuracy: 0.8286 - val_loss: 0.4227\n",
      "Epoch 5/40\n",
      "191/191 - 7s - 36ms/step - accuracy: 0.8828 - loss: 0.3071 - val_accuracy: 0.8070 - val_loss: 0.4553\n",
      "Epoch 6/40\n",
      "191/191 - 7s - 36ms/step - accuracy: 0.9097 - loss: 0.2493 - val_accuracy: 0.8030 - val_loss: 0.5036\n",
      "Epoch 7/40\n",
      "191/191 - 7s - 36ms/step - accuracy: 0.9337 - loss: 0.2005 - val_accuracy: 0.7873 - val_loss: 0.5853\n"
     ]
    }
   ],
   "source": [
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    {'text_input': X_train_seq, 'num_input': X_train_num},\n",
    "    y_train,\n",
    "    validation_data=(\n",
    "        {'text_input': X_val_seq, 'num_input': X_val_num},\n",
    "        y_val\n",
    "    ),\n",
    "    epochs=40,\n",
    "    batch_size=batch_size,      \n",
    "    callbacks=[early_stop],\n",
    "    verbose=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b34c63",
   "metadata": {},
   "source": [
    "Ah overfittea zarpado. No llega a 2 epochs y ya empieza a subir el loss de validación. Tengo que mejorarlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "67e9b6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 138ms/step\n",
      "Mejor threshold: 0.52 con F1 = 0.7922\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "y_val_proba = model.predict(\n",
    "    {'text_input': X_val_seq, 'num_input': X_val_num},\n",
    "    batch_size=256\n",
    ").ravel()\n",
    "\n",
    "thresholds = np.linspace(0.2, 0.8, 61)\n",
    "best_th, best_f1 = 0.5, 0\n",
    "\n",
    "for th in thresholds:\n",
    "    y_pred = (y_val_proba >= th).astype(int)\n",
    "    f1 = f1_score(y_val, y_pred)\n",
    "    if f1 > best_f1:\n",
    "        best_f1, best_th = f1, th\n",
    "\n",
    "print(f\"Mejor threshold: {best_th:.2f} con F1 = {best_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "719aa2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "F1-score en validación: 0.7900\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step\n",
      "F1 en entrenamiento: 0.8711\n",
      "F1 en validación: 0.7900\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Predicción en validación\n",
    "y_val_pred_proba = model.predict(\n",
    "    {'text_input': X_val_seq, 'num_input': X_val_num},\n",
    "    batch_size=batch_size\n",
    ").ravel()\n",
    "\n",
    "# Convertimos probabilidades a clases binarias (threshold 0.5)\n",
    "y_val_pred = (y_val_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "# Calculamos F1\n",
    "f1_val = f1_score(y_val, y_val_pred)\n",
    "print(f\"F1-score en validación: {f1_val:.4f}\")\n",
    "\n",
    "\n",
    "y_train_pred_proba = model.predict(\n",
    "    {'text_input': X_train_seq, 'num_input': X_train_num},\n",
    "    batch_size=batch_size\n",
    ").ravel()\n",
    "\n",
    "y_train_pred = (y_train_pred_proba >= 0.5).astype(int)\n",
    "f1_train = f1_score(y_train, y_train_pred)\n",
    "\n",
    "print(f\"F1 en entrenamiento: {f1_train:.4f}\")\n",
    "print(f\"F1 en validación: {f1_val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceac5f0b",
   "metadata": {},
   "source": [
    "Bueno, qué decir, la red neuronal con GRU entrenada con las features numéricas adicionales logró un F1 en validación de 0.79. Es lo mejor hasta ahora. No entendí todo la verdad, me encantaría profundizar más en redes neuronales y entender cada hiperparámetro, seguro podría mejorarlo mucho pero por ahora me quedo contento con este resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "218ac1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n",
      "Submission guardada en: ../resultados/red_neuronal_gru.csv\n"
     ]
    }
   ],
   "source": [
    "y_test_pred_proba = model.predict(\n",
    "    {'text_input': X_test_seq, 'num_input': X_test_num}\n",
    ").ravel()\n",
    "y_test_pred = (y_test_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'target': y_test_pred\n",
    "})\n",
    "\n",
    "from pathlib import Path\n",
    "submissions_dir = Path('../resultados')\n",
    "submissions_dir.mkdir(parents=True, exist_ok=True)\n",
    "submission_path = submissions_dir / 'red_neuronal_gru.csv'\n",
    "submission.to_csv(submission_path, index=False)\n",
    "print(f\"Submission guardada en: {submission_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
