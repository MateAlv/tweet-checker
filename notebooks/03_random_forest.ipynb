{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "547ec65d",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import f1_score \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "import json\n",
        "\n",
        "DATA_DIR = Path('../data/nlp-getting-started')\n",
        "TRAIN_PATH = DATA_DIR / 'train.csv'\n",
        "TEST_PATH = DATA_DIR / 'test.csv'\n",
        "LOCATION_TO_COUNTRY_PATH = Path('../data/location_to_country.json')\n",
        "RANDOM_SEED = 27\n",
        "\n",
        "train_df = pd.read_csv(TRAIN_PATH)\n",
        "test_df = pd.read_csv(TEST_PATH)\n",
        "\n",
        "with open(LOCATION_TO_COUNTRY_PATH, 'r', encoding='utf-8') as f:\n",
        "    location_to_country = json.load(f)\n",
        "\n",
        "train_df['country'] = train_df['location'].map(location_to_country)\n",
        "test_df['country'] = test_df['location'].map(location_to_country)\n",
        "\n",
        "print(f\"Train shape: {train_df.shape}\")\n",
        "print(f\"Test shape: {test_df.shape}\")\n",
        "print(f\"\\nCountries in train: {train_df['country'].notna().sum()}/{len(train_df)}\")\n",
        "print(f\"Countries in test: {test_df['country'].notna().sum()}/{len(test_df)}\")\n",
        "train_df.head()\n",
        "\n",
        "train_df['has_url'] = train_df['text'].fillna('').str.contains(r'http[s]?://', regex=True).astype(str)\n",
        "test_df['has_url'] = test_df['text'].fillna('').str.contains(r'http[s]?://', regex=True).astype(str)\n",
        "\n",
        "print(\"has_url distribution in train:\")\n",
        "print(train_df['has_url'].value_counts())\n",
        "print(f\"\\nPercentage with URL: {train_df['has_url'].eq('True').mean()*100:.1f}%\")\n",
        "\n",
        "print(\"\\nhas_url distribution in test:\")\n",
        "print(test_df['has_url'].value_counts())\n",
        "print(f\"\\nPercentage with URL: {test_df['has_url'].eq('True').mean()*100:.1f}%\")\n",
        "\n",
        "categorical_features = ['country', 'has_url']\n",
        "\n",
        "train_df['text_length'] = train_df['text'].fillna('').str.len()\n",
        "test_df['text_length'] = test_df['text'].fillna('').str.len()\n",
        "\n",
        "print(\"Text length stats:\")\n",
        "print(f\"Train - mean: {train_df['text_length'].mean():.1f}, std: {train_df['text_length'].std():.1f}\")\n",
        "print(f\"Test - mean: {test_df['text_length'].mean():.1f}, std: {test_df['text_length'].std():.1f}\")\n",
        "\n",
        "numeric_features = ['text_length']\n",
        "categorical_features = ['country', 'has_url']\n",
        "embedding_feature = 'text'\n",
        "\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "def get_sentiment(text):\n",
        "    if pd.isna(text) or text.strip() == '':\n",
        "        return 0.5\n",
        "    compound = analyzer.polarity_scores(text)['compound']\n",
        "    return (compound + 1) / 2\n",
        "\n",
        "train_df['sentiment_score'] = train_df['text'].apply(get_sentiment)\n",
        "test_df['sentiment_score'] = test_df['text'].apply(get_sentiment)\n",
        "\n",
        "print(\"Sentiment score stats:\")\n",
        "print(f\"Train - mean: {train_df['sentiment_score'].mean():.3f}, std: {train_df['sentiment_score'].std():.3f}\")\n",
        "print(f\"Test - mean: {test_df['sentiment_score'].mean():.3f}, std: {test_df['sentiment_score'].std():.3f}\")\n",
        "\n",
        "print(\"\\nSentiment score distribution in train:\")\n",
        "print(train_df['sentiment_score'].describe())\n",
        "\n",
        "numeric_features = ['text_length', 'sentiment_score']\n",
        "\n",
        "\n",
        "# 1. Separar features y target\n",
        "X = train_df[numeric_features + categorical_features + [embedding_feature]].copy()\n",
        "y = train_df['target'].copy()\n",
        "\n",
        "# 2. Split estratificado train/validation (80/20)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=RANDOM_SEED, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Train set: {len(X_train)} samples\")\n",
        "print(f\"Validation set: {len(X_val)} samples\")\n",
        "print(f\"Target distribution in train: {y_train.value_counts().to_dict()}\")\n",
        "print(f\"Target distribution in val: {y_val.value_counts().to_dict()}\")\n",
        "\n",
        "# Transformer para numéricas\n",
        "scaler = StandardScaler()\n",
        "X_train_numeric = scaler.fit_transform(X_train[numeric_features])\n",
        "X_val_numeric = scaler.transform(X_val[numeric_features])\n",
        "X_test_numeric = scaler.transform(test_df[numeric_features])\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "# has_url -> OneHotEncoder\n",
        "ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=True)\n",
        "\n",
        "X_train_hasurl = ohe.fit_transform(\n",
        "    X_train[['has_url']].fillna('missing')\n",
        ")\n",
        "X_val_hasurl = ohe.transform(\n",
        "    X_val[['has_url']].fillna('missing')\n",
        ")\n",
        "X_test_hasurl = ohe.transform(\n",
        "    test_df[['has_url']].fillna('missing')\n",
        ")\n",
        "\n",
        "print(\"\\nOneHotEncoder (has_url):\")\n",
        "print(f\"  categories: {ohe.categories_[0].tolist()}\")\n",
        "\n",
        "\n",
        "# country -> Mean Target Encoding con K-Fold\n",
        "country_col = 'country'\n",
        "alpha = 10  # smoothing\n",
        "global_mean = y_train.mean()\n",
        "\n",
        "# aseguramos string y categoría \"missing\"\n",
        "country_train = X_train[country_col].fillna('missing').astype(str)\n",
        "country_val = X_val[country_col].fillna('missing').astype(str)\n",
        "country_test = test_df[country_col].fillna('missing').astype(str)\n",
        "\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
        "\n",
        "country_train_enc = pd.Series(index=country_train.index, dtype=float)\n",
        "\n",
        "for train_idx, holdout_idx in kfold.split(country_train):\n",
        "    # folds internos para evitar leakage\n",
        "    ct_train = country_train.iloc[train_idx]\n",
        "    y_fold = y_train.iloc[train_idx]\n",
        "\n",
        "    stats = (\n",
        "        pd.DataFrame({'country': ct_train, 'target': y_fold})\n",
        "        .groupby('country')['target']\n",
        "        .agg(['mean', 'count'])\n",
        "    )\n",
        "    stats['smoothed'] = (\n",
        "        stats['mean'] * stats['count'] + alpha * global_mean\n",
        "    ) / (stats['count'] + alpha)\n",
        "\n",
        "    enc_values = country_train.iloc[holdout_idx].map(stats['smoothed'])\n",
        "    country_train_enc.iloc[holdout_idx] = enc_values\n",
        "\n",
        "# categorías raras que no se mapearon en algún fold\n",
        "country_train_enc = country_train_enc.fillna(global_mean)\n",
        "\n",
        "# encoding para val/test: se calcula con TODO el train (ya no hay leakage)\n",
        "full_stats = (\n",
        "    pd.DataFrame({'country': country_train, 'target': y_train})\n",
        "    .groupby('country')['target']\n",
        "    .agg(['mean', 'count'])\n",
        ")\n",
        "full_stats['smoothed'] = (\n",
        "    full_stats['mean'] * full_stats['count'] + alpha * global_mean\n",
        ") / (full_stats['count'] + alpha)\n",
        "\n",
        "country_val_enc = country_val.map(full_stats['smoothed']).fillna(global_mean)\n",
        "country_test_enc = country_test.map(full_stats['smoothed']).fillna(global_mean)\n",
        "\n",
        "print(\"\\nMean encoding (country):\")\n",
        "print(f\"  global_mean: {global_mean:.4f}\")\n",
        "print(f\"  ejemplo valores train: {country_train_enc.head().to_dict()}\")\n",
        "\n",
        "# Pasar estos vectores a matrices sparse columna para combinarlos con hstack\n",
        "X_train_country_enc = csr_matrix(country_train_enc.values.reshape(-1, 1))\n",
        "X_val_country_enc   = csr_matrix(country_val_enc.values.reshape(-1, 1))\n",
        "X_test_country_enc  = csr_matrix(country_test_enc.values.reshape(-1, 1))\n",
        "\n",
        "# matriz categórica final = [has_url OHE, country_mean]\n",
        "X_train_cat = hstack([X_train_hasurl, X_train_country_enc])\n",
        "X_val_cat   = hstack([X_val_hasurl,   X_val_country_enc])\n",
        "X_test_cat  = hstack([X_test_hasurl,  X_test_country_enc])\n",
        "\n",
        "\n",
        "# Transformer para texto (TF-IDF)\n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    max_features=5000,\n",
        "    ngram_range=(1, 2),\n",
        "    min_df=2,\n",
        "    max_df=0.95,\n",
        "    strip_accents='unicode',\n",
        "    lowercase=True,\n",
        "    analyzer='word',\n",
        "    token_pattern=r'\\w{1,}',\n",
        "    stop_words='english'\n",
        ")\n",
        "X_train_text = tfidf_vectorizer.fit_transform(X_train[embedding_feature].fillna(''))\n",
        "X_val_text = tfidf_vectorizer.transform(X_val[embedding_feature].fillna(''))\n",
        "X_test_text = tfidf_vectorizer.transform(test_df[embedding_feature].fillna(''))\n",
        "\n",
        "print(f\"\\nTF-IDF vectorizer:\")\n",
        "print(f\"  Vocabulary size: {len(tfidf_vectorizer.vocabulary_)}\")\n",
        "print(f\"  Feature names (first 10): {tfidf_vectorizer.get_feature_names_out()[:10].tolist()}\")\n",
        "\n",
        "\n",
        "# 4. Combinar todas las features\n",
        "X_train_combined = hstack([X_train_numeric, X_train_cat, X_train_text])\n",
        "X_val_combined = hstack([X_val_numeric, X_val_cat, X_val_text])\n",
        "X_test_combined = hstack([X_test_numeric, X_test_cat, X_test_text])\n",
        "\n",
        "print(f\"\\nCombined feature matrix:\")\n",
        "print(f\"  Train shape: {X_train_combined.shape}\")\n",
        "print(f\"  Validation shape: {X_val_combined.shape}\")\n",
        "print(f\"  Test shape: {X_test_combined.shape}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75634525",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
