{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "74f0d1e6",
      "metadata": {},
      "source": [
        "# 07 ¬∑ Feature Score de LLM Local Qwen\n",
        "\n",
        "Me qued√© pensando, esta clasificaci√≥n suena como una tarea que har√≠a muy bien un LLM. Medio que ya hacemos eso con los embeddings, pero capaz que puede aportar algo la feature del LLM score. Probemos!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "51300ee4",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "try:\n",
        "    from tqdm.auto import tqdm\n",
        "except ImportError:  \n",
        "    def tqdm(iterable, **kwargs):\n",
        "        return iterable\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "1e0e09bb",
      "metadata": {},
      "outputs": [],
      "source": [
        "REPO_ROOT = Path.cwd()\n",
        "if not (REPO_ROOT / 'data').exists():\n",
        "    REPO_ROOT = REPO_ROOT.parent\n",
        "\n",
        "DATA_DIR = REPO_ROOT / 'data' / 'nlp-getting-started'\n",
        "TRAIN_PATH = DATA_DIR / 'train.csv'\n",
        "TEST_PATH = DATA_DIR / 'test.csv'\n",
        "\n",
        "OUTPUT_DIR = REPO_ROOT / 'resultados' / 'llm_features'\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "train_df = pd.read_csv(TRAIN_PATH)\n",
        "test_df = pd.read_csv(TEST_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71fc5ade",
      "metadata": {},
      "source": [
        "## Prompt usado:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "2526d782",
      "metadata": {},
      "outputs": [],
      "source": [
        "PROMPT_TEMPLATE = \"\"\"You are a classifier for a Kaggle competition.\n",
        "\n",
        "Task:\n",
        "Given the text of a tweet, estimate how likely it is that the tweet describes a REAL disaster\n",
        "(real fire, flood, earthquake, accident, explosion, etc.) rather than a metaphor, joke, or\n",
        "unrelated content.\n",
        "\n",
        "Respond with a single number between 0 and 1 (inclusive):\n",
        "- 0 means \"definitely NOT about a real disaster\"\n",
        "- 1 means \"definitely about a real disaster\"\n",
        "\n",
        "Examples:\n",
        "Tweet: \"Forest fire near La Ronge Sask. Canada\"\n",
        "Answer: 0.82\n",
        "\n",
        "Tweet: \"My phone is on fire from all these messages lol\"\n",
        "Answer: 0.03\n",
        "\n",
        "Tweet: \"Car accident on the highway blocking both lanes, emergency services on scene\"\n",
        "Answer: 0.91\n",
        "\n",
        "Tweet: \"This party is so lit I'm dying üòÇ\"\n",
        "Answer: 0.01\n",
        "\n",
        "Output ONLY the number, with no explanation.\n",
        "\n",
        "Tweet:\n",
        "{text}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fb63b13",
      "metadata": {},
      "source": [
        "## Par√°metros del prompt y Ollama\n",
        "Me parece que lo mejor para esto es temp 0. Mi idea es que no se ponga muy creativo. Mismo concepto con top p. Repeat penalty lo dejo en ese default, y num predict bajo para que no se extienda mucho, quiero solo el score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "abb94026",
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import requests\n",
        "\n",
        "OLLAMA_BASE_URL = os.environ.get('OLLAMA_URL', 'http://localhost:11434')\n",
        "OLLAMA_MODEL = os.environ.get('OLLAMA_MODEL', 'qwen2.5:7b-instruct')\n",
        "OLLAMA_OPTIONS = {\n",
        "    'temperature': 0.0,\n",
        "    'top_p': 0.0,\n",
        "    'repeat_penalty': 1.0,\n",
        "    'num_predict': 8,\n",
        "}\n",
        "\n",
        "FLOAT_RE = re.compile(r'([01](?:\\.\\d+)?|\\d?\\.\\d+)')\n",
        "\n",
        "def llm_disaster_score(text: str) -> float:\n",
        "    prompt = PROMPT_TEMPLATE.format(text=text)\n",
        "\n",
        "    payload = {\n",
        "        \"model\": OLLAMA_MODEL,\n",
        "        \"prompt\": prompt,\n",
        "        \"stream\": False,\n",
        "        \"options\": OLLAMA_OPTIONS,\n",
        "    }\n",
        "\n",
        "    r = requests.post(f\"{OLLAMA_BASE_URL}/api/generate\", json=payload)\n",
        "    r.raise_for_status()\n",
        "\n",
        "    resp = r.json()[\"response\"].strip()\n",
        "    m = FLOAT_RE.search(resp)\n",
        "    if not m:\n",
        "        return 0.5\n",
        "\n",
        "    try:\n",
        "        val = float(m.group(1).strip('\"'))\n",
        "    except:\n",
        "        val = 0.5\n",
        "\n",
        "    return max(0.0, min(1.0, val))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e08ffd9",
      "metadata": {},
      "source": [
        "Probemos el prompt con unos ejemplos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "289ef357",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.95\n"
          ]
        }
      ],
      "source": [
        "print(llm_disaster_score(\"Forest fire spotted in California. Evacuations starting.\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c407f4d",
      "metadata": {},
      "source": [
        "Buen√≠simo, me gust√≥ el score para ese tweet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "7aad76a7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.01\n"
          ]
        }
      ],
      "source": [
        "print(llm_disaster_score(\"Lol last night's the strokes concert was so fucking lit, OMFG\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d451f298",
      "metadata": {},
      "source": [
        "Jajajaja le doy un aprobado personalmente. Ya soy un prompt engineer (? Bueno pasemosle todo el dataset entonces y guardemos el output en un JSON."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "69c2d67c",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "LLM scores train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7613/7613 [17:07<00:00,  7.41it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved 7613 scores to /home/mate/FIUBA/ciencia-de-datos/tweet-checker/resultados/llm_features/llm_scores_train.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "LLM scores test: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3263/3263 [07:18<00:00,  7.43it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved 3263 scores to /home/mate/FIUBA/ciencia-de-datos/tweet-checker/resultados/llm_features/llm_scores_test.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "\n",
        "def compute_split_scores(df: pd.DataFrame, split_name: str, output_dir: Path):\n",
        "    scores = {}\n",
        "    for _, row in tqdm(df.iterrows(), total=len(df), desc=f\"LLM scores {split_name}\"):\n",
        "        tweet_id = int(row[\"id\"])\n",
        "        text = row.get(\"text\", \"\")\n",
        "        try:\n",
        "            score = llm_disaster_score(text if isinstance(text, str) else \"\")\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] Error scoring id={tweet_id}: {e}\")\n",
        "            score = 0.5  # Si no anduvo le pongo 0.5, indefinido.\n",
        "\n",
        "        if not isinstance(score, (int, float)) or not math.isfinite(score):\n",
        "            score = 0.5\n",
        "        score = max(0.0, min(1.0, float(score)))\n",
        "\n",
        "        scores[str(tweet_id)] = score\n",
        "\n",
        "    out_path = output_dir / f\"llm_scores_{split_name}.json\"\n",
        "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(scores, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"Saved {len(scores)} scores to {out_path}\")\n",
        "\n",
        "\n",
        "# === Ejecutar para train y test ===\n",
        "compute_split_scores(train_df, \"train\", OUTPUT_DIR)\n",
        "compute_split_scores(test_df, \"test\", OUTPUT_DIR)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
